{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "jax.device_count(), jax.devices()[0].device_kind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper_stream.core.helpers.data_loading import load_data_samples_from_path\n",
    "from whisper_stream.pipelines.jax_pipelines import (\n",
    "    JAXStreamingPipeline,\n",
    ")\n",
    "from whisper_stream.pipelines.jax_pipelines.constants import (\n",
    "    JAXValidDtypesMapping,\n",
    "    JAXScalarDType,\n",
    ")\n",
    "\n",
    "from whisper_stream.core.constants import WhisperValidCheckpoints, WhisperValidTasks\n",
    "\n",
    "from whisper_stream.core.logger import LOG_LEVEL_NAMES\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare\n",
    "checkpoint: WhisperValidCheckpoints = \"openai/whisper-tiny\"\n",
    "model_dtype: JAXScalarDType = JAXValidDtypesMapping[\"BFLOAT16\"]\n",
    "task: WhisperValidTasks = \"transcribe\"\n",
    "language: str = \"english\"\n",
    "return_timestamps: bool = True\n",
    "batch_size: int = 1\n",
    "log_level: LOG_LEVEL_NAMES = \"INFO\"\n",
    "\n",
    "data_directory = Path(\"../data\")\n",
    "\n",
    "run_opts = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"return_timestamps\": return_timestamps,\n",
    "    \"language\": language,\n",
    "    \"task\": task,\n",
    "}\n",
    "\n",
    "# construct\n",
    "pipeline = JAXStreamingPipeline(\n",
    "    checkpoint=checkpoint,\n",
    "    dtype=model_dtype,\n",
    "    batch_size=batch_size,\n",
    "    min_log_level=log_level,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "pipeline_data: bytes = load_data_samples_from_path(\n",
    "    \"audio_2.mp3\", directory=data_directory, binary_mode=True\n",
    ")  # 4s\n",
    "pipeline_data_large: bytes = load_data_samples_from_path(\n",
    "    \"tryst.mp3\", directory=data_directory, binary_mode=True\n",
    ")  # 4:44s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize & warmup\n",
    "%time pipeline.initialize_pipeline(**run_opts, use_experimental_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be warmed up now (time should be similar to # small data)\n",
    "%time list(pipeline(pipeline_data, **run_opts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small data\n",
    "%time list(pipeline(pipeline_data, **run_opts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small data in batch\n",
    "%time list(pipeline([pipeline_data] * 10, **run_opts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunkable data\n",
    "%time list(pipeline(pipeline_data_large, **run_opts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunkable data in batches\n",
    "%time list(pipeline([pipeline_data_large] * 32, **run_opts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_mode_data: list[bytes] = [\n",
    "    pipeline_data_large,\n",
    "    pipeline_data,\n",
    "    pipeline_data,\n",
    "    pipeline_data,\n",
    "] * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed data, received as it comes, using default `smallest` strategy the smaller files will come in larger batches first\n",
    "start: float = time.time()\n",
    "for data in pipeline(mixed_mode_data, strategy=\"smallest\", **run_opts):\n",
    "    print({\"num_items\": len(data)}, end=\"\\n\")\n",
    "    print({\"data\": data, \"time_taken\": f\"{time.time() - start:.2}s\"}, end=\"\\n\")\n",
    "    print(\"-\" * 40, end=\"\\n\")\n",
    "    start = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
